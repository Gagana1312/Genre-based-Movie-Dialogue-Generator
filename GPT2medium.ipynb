{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1NVqCgPrITpyksrJxBxXyd9KAsH-aCssZ","authorship_tag":"ABX9TyNX3lqLSwpGaDnOrzlX9gEv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sBy6GoQorOVA","executionInfo":{"status":"ok","timestamp":1691540626149,"user_tz":240,"elapsed":20390,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"3d939a54-0500-4b70-a1a7-61e0444baa9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"]}],"source":["pip install transformers"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZFobPqf0r7sW","executionInfo":{"status":"ok","timestamp":1691540797495,"user_tz":240,"elapsed":7372,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"493abb61-215b-4d41-cb30-e34660e9aaa7"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","from nltk.translate.bleu_score import corpus_bleu\n","from torch.nn.functional import cross_entropy\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n"],"metadata":{"id":"n_Xb7P5brYy1","executionInfo":{"status":"ok","timestamp":1691544831702,"user_tz":240,"elapsed":288,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["\n","# Load and preprocess the dataset\n","file_path = '/content/drive/MyDrive/genre.csv'\n","data = pd.read_csv(file_path)\n","data = data.dropna()\n","conversations = data['text'].tolist()\n",""],"metadata":{"id":"yDN5DmjX7c6-","executionInfo":{"status":"ok","timestamp":1691544863604,"user_tz":240,"elapsed":2459,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Split the dataset into train and test sets\n","train_conversations, test_conversations = train_test_split(conversations, test_size=0.2, random_state=42)\n","\n","# Pre-trained model and tokenizer\n","model_name = 'gpt2-medium'\n","model = GPT2LMHeadModel.from_pretrained(model_name, pad_token_id=50256)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",""],"metadata":{"id":"lmy5ntsf7kwb","executionInfo":{"status":"ok","timestamp":1691544911791,"user_tz":240,"elapsed":10419,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Generate a single response\n","def generate_response(input_text, max_length=100, temperature=1.0, top_k=50):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","    attention_mask = torch.ones_like(input_ids)\n","    with torch.no_grad():\n","        output = model.generate(input_ids, attention_mask=attention_mask,\n","                                max_length=max_length, temperature=temperature,\n","                                top_k=top_k)\n","    response = tokenizer.decode(output[0], skip_special_tokens=True)\n","    return response\n"],"metadata":{"id":"ZWgnWGd37vfT","executionInfo":{"status":"ok","timestamp":1691544935380,"user_tz":240,"elapsed":289,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# post-processing\n","def post_process_response(response):\n","    # Filter out overly long responses\n","    response = \" \".join(response.split()[:50])\n","\n","    # Capitalize the first letter\n","    response = response.capitalize()\n","\n","    return response"],"metadata":{"id":"zWgTeisy72Rj","executionInfo":{"status":"ok","timestamp":1691544962942,"user_tz":240,"elapsed":359,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\n","# Store reference responses and bot responses\n","reference_responses = []\n","bot_responses = []\n","train_references = []\n","train_hypotheses = []\n","test_references = []\n","test_hypotheses = []\n","\n","# Initialize empty lists to store training and testing input IDs\n","train_input_ids = []\n","test_input_ids = []\n","\n","def simulate_conversation(conversations_list, references_list, hypotheses_list):\n","    # Iterate through conversations and generate responses\n","    for conversation in conversations_list:\n","        context_window = []\n","\n","        while True:\n","            user_input = input(\"You: \")  # Take user input\n","\n","            if user_input.strip().lower() == 'x':\n","                break\n","\n","            context_window.append(\"You: \" + user_input)\n","            if len(context_window) > 5:\n","                context_window.pop(0)\n","\n","            prompt = context_window[-1][4:]\n","            response = generate_response(prompt, max_length=150, temperature=0.8, top_k=50)\n","            response_sentence = response.split('.')[0]\n","            response_sentence = post_process_response(response_sentence)\n","\n","            print(\"Bot:\", response_sentence.strip())\n","\n","            # Append to references and hypotheses lists\n","            references_list.append(user_input)\n","            hypotheses_list.append(response_sentence)\n","            reference_responses.append(response_sentence)\n","            bot_responses.append(response_sentence)\n","\n","        exit_loop_input = input(\"Enter 'exit' to stop iterating through conversations, or press Enter to continue: \")\n","        if exit_loop_input.strip().lower() == 'exit':\n","            print(\"Exiting conversation loop.\")\n","            break\n","\n","\n","simulate_conversation(train_conversations, train_references, train_hypotheses)\n","simulate_conversation(test_conversations, test_references, test_hypotheses)\n","\n","# Calculate BLEU scores\n","references = [[ref.split()] for ref in reference_responses]\n","hypotheses = [response.split() for response in bot_responses]\n","\n","bleu_score = corpus_bleu(references, hypotheses)\n","print(f\"Overall BLEU Score: {bleu_score:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpTgO_c578qX","executionInfo":{"status":"ok","timestamp":1691546591338,"user_tz":240,"elapsed":1147318,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"f5afa300-f1df-49d3-d55d-f2803e141a59"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["You: fact\n","Bot: Fact that the government has been able to keep the public in the dark about the extent of the nsa's surveillance programs is a testament to the government's ability to keep the public in the dark about its own activities\n","You: back\n","Bot: Back to the beginning of the game\n","You: option\n","Bot: Option to use the same password for both accounts\n","You: train\n","Bot: Train\n","You: actual target\n","Bot: Actual target\n","You: take care\n","Bot: Take care of the rest\n","You: ensure the\n","Bot: Ensure the safety of the public\n","You: I apologize\n","Bot: I apologize for the delay in posting this, but i have been working on a new project and i have been unable to get it done\n","You: Let's try\n","Bot: Let's try to get a sense of what's going on here\n","You: seems to\n","Bot: Seems to be a very good thing\n","You: which is\n","Bot: Which is a very good thing\n","You: This is the\n","Bot: This is the first time that the government has been able to use the power of the purse to force a company to pay back money it owes\n","You: has been able\n","Bot: Has been able to get a lot of people to pay attention to the issue\n","You: performance\n","Bot: Performance of the program\n","You: irrelevant\n","Bot: Irrelevant to the question of whether the government has the authority to compel the production of records\n","You: assessment\n","Bot: Assessment of the impact of the proposed changes on the health of the public\n","You: x\n","Enter 'exit' to stop iterating through conversations, or press Enter to continue: exit\n","Exiting conversation loop.\n","You: significant\n","Bot: Significant impact on the economy\n","You: company\n","Bot: Company's website\n","You: compare\n","Bot: Compare to the other two\n","You: directly\n","Bot: Directly to the user\n","You: literature\n","Bot: Literature, and the internet\n","You: evaluating\n","Bot: Evaluating the impact of the proposed changes on the health of the community\n","You: based on\n","Bot: Based on the number of people who have been killed in the conflict\n","You: publish\n","Bot: Publish a report on the issue\n","You: if your tast\n","Bot: If your tastebuds are not yet ready to accept the idea of a vegan diet, you can still enjoy the benefits of a plant-based diet\n","You: remember\n","Bot: Remember that the first time i saw the movie, i was in my early 20s, and i was so excited to see it\n","You: much in the\n","Bot: Much in the way of a \"good\" or \"bad\" story\n","You: related to\n","Bot: Related to the use of the term \"sexually violent predator\" in the criminal code\n","You: such as a\n","Bot: Such as a \"bump\" in the road, a \"slip\" in the road, or a \"slip\" in the road\n","You: this can help\n","Bot: This can help you to understand the difference between the two\n","You: understand it\n","Bot: Understand it\n","You: New chat\n","Bot: New chat with the team\n","You: x\n","Enter 'exit' to stop iterating through conversations, or press Enter to continue: exit\n","Exiting conversation loop.\n","Overall BLEU Score: 0.9922\n"]}]},{"cell_type":"code","source":["\n","# Calculate Cross-Entropy Loss for Training and Test Set Responses\n","def calculate_loss(logits, target_ids):\n","    loss = torch.nn.functional.cross_entropy(logits, target_ids, reduction='sum')\n","    return loss\n","\n","total_train_loss = 0.0\n","num_train_tokens = 0\n","\n","total_test_loss = 0.0\n","num_test_tokens = 0\n","\n","# For both training and test sets\n","for references, hypotheses in zip([train_references, test_references], [train_hypotheses, test_hypotheses]):\n","    for reference, hypothesis in zip(references, hypotheses):\n","        reference_ids = [tokenizer.convert_tokens_to_ids(ref) for ref in reference.split()]\n","        hypothesis_ids = [tokenizer.convert_tokens_to_ids(hyp) for hyp in hypothesis.split()]\n","\n","        # Ensure that reference and hypothesis have the same length\n","        min_length = min(len(reference_ids), len(hypothesis_ids))\n","        reference_ids = reference_ids[:min_length]\n","        hypothesis_ids = hypothesis_ids[:min_length]\n","\n","        reference_tensor = torch.tensor(reference_ids).unsqueeze(0)\n","        hypothesis_tensor = torch.tensor(hypothesis_ids).unsqueeze(0)\n","\n","        # Get logits\n","        reference_logits = model(input_ids=reference_tensor).logits.squeeze(0)\n","        hypothesis_logits = model(input_ids=hypothesis_tensor).logits.squeeze(0)\n","\n","        # Calculate loss for both reference and hypothesis\n","        train_loss = calculate_loss(reference_logits, torch.tensor(hypothesis_ids))\n","        test_loss = calculate_loss(hypothesis_logits, torch.tensor(hypothesis_ids))\n","\n","        total_train_loss += train_loss.item()\n","        total_test_loss += test_loss.item()\n","\n","        num_train_tokens += len(hypothesis_ids)\n","        num_test_tokens += len(hypothesis_ids)\n","\n","average_train_loss = total_train_loss / num_train_tokens\n","average_test_loss = total_test_loss / num_test_tokens\n","\n","print(f\"Average Cross-Entropy Loss (Training Set): {average_train_loss:.4f}\")\n","print(f\"Average Cross-Entropy Loss (Test Set): {average_test_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UsO7Md8ACLLA","executionInfo":{"status":"ok","timestamp":1691546627890,"user_tz":240,"elapsed":19757,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"b7e310de-77aa-45cf-f5fc-dbe8d882fd9c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Average Cross-Entropy Loss (Training Set): 10.4506\n","Average Cross-Entropy Loss (Test Set): 10.1373\n"]}]},{"cell_type":"code","source":["# Save the trained model\n","output_dir = '/content/drive/MyDrive/NLP_trained_chatbot_model'\n","model.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Trained model and tokenizer saved to {output_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V6y0rTTaCTz0","executionInfo":{"status":"ok","timestamp":1691546653898,"user_tz":240,"elapsed":20639,"user":{"displayName":"Gagana Ananda","userId":"13815533908099765323"}},"outputId":"f4c827b0-c1f3-4c20-8feb-3cfb0ad23dd8"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Trained model and tokenizer saved to /content/drive/MyDrive/NLP_trained_chatbot_model\n"]}]}]}